{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a4ac7-bbc7-4b5a-93f8-b4c2a2716b58",
   "metadata": {},
   "source": [
    "# Obtaining basic statistics for textual lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d62a9927-f2fc-440c-b877-1a77aeade405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yadasa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/yadasa/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "# import talib as ta\n",
    "# import talib\n",
    "import nltk \n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58e170-7f53-4e9b-b3da-fafa1fa1a3ce",
   "metadata": {},
   "source": [
    "# Project Planning - EDA & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fda116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/raw_analyst_ratings.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1213658-409d-4fbf-b3fe-d72966482e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18e761-800e-4cb8-9397-878df59a5a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the length of each headline and create a new column\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(x))\n",
    "\n",
    "# Basic statistics for headline length\n",
    "headline_stats = df['headline_length'].describe()\n",
    "\n",
    "# Print the statistics\n",
    "print(\"\\nStatistics of headlines\")\n",
    "\n",
    "headline_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c57249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "# Extract relevant date components for analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.weekday  \n",
    "# Monday is 0, Sunday is 6\n",
    "\n",
    "# Analyze trends over time, such as news frequency by year, month, day, and weekday\n",
    "news_frequency_by_year = df['year'].value_counts().sort_index()\n",
    "news_frequency_by_month = df.groupby('month')['headline'].count()\n",
    "news_frequency_by_day = df.groupby('day')['headline'].count()\n",
    "news_frequency_by_weekday = df.groupby('weekday')['headline'].count()\n",
    "\n",
    "# Print descriptive output\n",
    "print(\"News frequency by year:\")\n",
    "print(news_frequency_by_year)\n",
    "print(\"\\nExplanation: This shows the count of news articles published each year.\")\n",
    "\n",
    "print(\"\\nNews frequency by month:\")\n",
    "print(news_frequency_by_month)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each month of the year.\")\n",
    "\n",
    "print(\"\\nNews frequency by day:\")\n",
    "print(news_frequency_by_day)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the month.\")\n",
    "\n",
    "print(\"\\nNews frequency by weekday:\")\n",
    "print(news_frequency_by_weekday)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the week (Monday to Sunday). Monday is represented as 0, Sunday as 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70e907-ec31-4e73-bf40-5b1b0efb403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e347b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "-------------------------\n",
      "Neutral: 741194 headlines\n",
      "Positive: 441858 headlines\n",
      "Negative: 224276 headlines\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score for each headline\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Apply the function to each headline and store the results in a new column\n",
    "df['sentiment'] = df['headline'].apply(get_sentiment_score)\n",
    "\n",
    "# Define a function to classify the sentiment based on the compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to the compound score and store the results in a new column\n",
    "df['sentiment_class'] = df['sentiment'].apply(lambda x: classify_sentiment(x['compound']))\n",
    "\n",
    "# Count the number of headlines per sentiment class\n",
    "sentiment_counts = df['sentiment_class'].value_counts()\n",
    "\n",
    "# Print the sentiment counts in a more descriptive format\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"-------------------------\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} headlines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000  # Increase max_length to handle longer texts\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Set chunk size\n",
    "chunk_size = 100000  # Adjust as needed\n",
    "\n",
    "# Function to process text in chunks\n",
    "def process_text_chunks(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Perform part-of-speech tagging to extract nouns and proper nouns\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "    # Use spaCy for named entity recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Filter entities to extract only significant ones\n",
    "    significant_entities = [entity[0] for entity in entities if entity[1] in ['ORG', 'PERSON', 'GPE', 'EVENT']]\n",
    "\n",
    "    # Combine both noun phrases and significant named entities\n",
    "    keywords = nouns + significant_entities\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Concatenate all headlines into a single string\n",
    "all_headlines = ' '.join(df['headline'])\n",
    "\n",
    "# Process the text in chunks\n",
    "chunks = [all_headlines[i:i+chunk_size] for i in range(0, len(all_headlines), chunk_size)]\n",
    "\n",
    "# Initialize a list to store keywords from all chunks\n",
    "all_keywords = []\n",
    "\n",
    "# Process each chunk separately\n",
    "for chunk in chunks:\n",
    "    keywords = process_text_chunks(chunk)\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "# Calculate the frequency distribution of keywords\n",
    "fdist = FreqDist(all_keywords)\n",
    "\n",
    "# Get the most common keywords\n",
    "common_keywords = fdist.most_common(10)\n",
    "\n",
    "# Print the most common keywords\n",
    "print(\"Most common keywords or phrases:\")\n",
    "for keyword, frequency in common_keywords:\n",
    "    print(f\"{keyword}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250df10-2ec1-4979-a8c0-b47d9d567537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the data to get the count of articles per day\n",
    "article_count_per_day = df.resample('D').size()\n",
    "\n",
    "# Plot the publication frequency over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "article_count_per_day.plot()\n",
    "plt.title('Publication Frequency Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9658-3fdf-4d64-8975-d221c376eaf3",
   "metadata": {},
   "source": [
    " # Analysis of publishing times might reveal if there’s a specific time when most news is released, which could be crucial for traders and automated trading systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fae66-03ad-43df-bc42-edba8455f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with a column named 'date' containing the publication dates.\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "\n",
    "# Extract the hour of the day from the 'date' column\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Count the number of articles published during each hour\n",
    "article_count_by_hour = df.groupby('hour').size()\n",
    "\n",
    "# Plot the distribution of publishing times\n",
    "plt.figure(figsize=(10, 6))\n",
    "article_count_by_hour.plot(kind='bar')\n",
    "plt.title('Distribution of Publishing Times')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(range(24), rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc40c25-a3ab-4caf-a389-1ec53908eafe",
   "metadata": {},
   "source": [
    "# Which publishers contribute most to the news feed? Is there a difference in the type of news they report?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ff4f-53e8-4953-86b3-a3b41dd04faf",
   "metadata": {},
   "source": [
    "# To answer the above two question, we use \n",
    "# 1)findind number of article publishe by each publisher\n",
    "# 2)Number of stop words in each publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902a9bb-2bbe-421f-a574-79647b7a8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Count the number of articles published by each publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# Sort publishers by the number of articles published (in ascending order)\n",
    "sorted_publishers = publisher_counts.sort_values().index\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and process the headlines to identify common keywords\n",
    "keywords_by_publisher = {}\n",
    "for publisher in sorted_publishers:\n",
    "    publisher_data = df[df['publisher'] == publisher]\n",
    "    all_headlines = ' '.join(publisher_data['headline'])\n",
    "    tokens = word_tokenize(all_headlines)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    keywords_by_publisher[publisher] = fdist.most_common(10)  # Get top 10 keywords for each publisher\n",
    "\n",
    "# Print the number of articles published by each publisher\n",
    "print(\"Number of articles published by each publisher (in ascending order):\")\n",
    "print(publisher_counts)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the top keywords associated with each publisher\n",
    "print(\"Top keywords associated with each publisher:\")\n",
    "for publisher in sorted_publishers:\n",
    "    print(f\"Publisher: {publisher}\")\n",
    "    keywords = keywords_by_publisher.get(publisher, [])\n",
    "    for keyword, frequency in keywords:\n",
    "        print(f\"{keyword}: {frequency}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a23c9-792a-435d-b1b1-ab5cf192e0fd",
   "metadata": {},
   "source": [
    "# What if the publisher is an Orginization which contributes most to news?\n",
    "# if eamil is used as publisher name we can extract unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb165f-1cf4-46fe-a839-b44094bf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from email addresses\n",
    "df['domain'] = df['publisher'].str.extract(r'@(.*)')\n",
    "\n",
    "# Count the number of articles published by each domain\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "# Print the unique domains and their corresponding publication counts\n",
    "print(\"Unique domains and their publication counts:\")\n",
    "print(domain_counts)\n",
    "\n",
    "# Find the domain with the highest number of articles published\n",
    "most_contributor_domain = domain_counts.idxmax()\n",
    "articles_published = domain_counts.max()\n",
    "\n",
    "# Print the most contributing domain\n",
    "print(f\"The most contributing domain is '{most_contributor_domain}' with {articles_published} articles published.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f7457-7de2-48dc-827d-ac954723b32f",
   "metadata": {},
   "source": [
    "# The most contributing domain is 'benzinga.com' with 7937 articles published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841dfac-6951-43b0-b802-6564aa134617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Load the dataset containing stock symbols and dates from a CSV file into a pandas DataFrame\n",
    "\n",
    "# Convert the 'date' column to datetime objects to facilitate date manipulation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Determine date ranges and fetch stock data\n",
    "date_ranges = df.groupby('symbol').agg({'date': ['min', 'max']})\n",
    "\n",
    "# Function to fetch stock data for a given symbol and date range\n",
    "def fetch_stock_data(symbol, start_date, end_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through each symbol and fetch stock data based on the calculated date ranges\n",
    "for symbol, (start_date, end_date) in date_ranges.iterrows():\n",
    "    print(f\"Fetching data for {symbol} from {start_date} to {end_date}\")\n",
    "    stock_data = fetch_stock_data(symbol, start_date, end_date)\n",
    "    stock_data\n",
    "    \n",
    "    # Process fetched data (you can save it, manipulate it, etc.)\n",
    "    if stock_data is not None:\n",
    "        # Perform operations on the fetched data here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc8c63-a7f9-4aa0-ac56-368897e3bab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7dad9-f7dd-454d-9866-75af71f3960a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca7ab9-8d8a-4576-a5a7-0dd65c2210a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df1a14-8176-4331-9f8f-e2acf44a76a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
