{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a4ac7-bbc7-4b5a-93f8-b4c2a2716b58",
   "metadata": {},
   "source": [
    "# Obtaining basic statistics for textual lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a9927-f2fc-440c-b877-1a77aeade405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib as ta\n",
    "import talib\n",
    "import nltk \n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5a54e-3a92-4f7c-9b6f-780ea9f6fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58e170-7f53-4e9b-b3da-fafa1fa1a3ce",
   "metadata": {},
   "source": [
    "# Project Planning - EDA & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96fda116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yadasa/Desktop/Finance-Data-Analysis/data/raw_analyst_ratings.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1213658-409d-4fbf-b3fe-d72966482e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd18e761-800e-4cb8-9397-878df59a5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1.407328e+06\n",
      "mean     7.312051e+01\n",
      "std      4.073531e+01\n",
      "min      3.000000e+00\n",
      "25%      4.700000e+01\n",
      "50%      6.400000e+01\n",
      "75%      8.700000e+01\n",
      "max      5.120000e+02\n",
      "Name: headline_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the length of each headline and create a new column\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(x))\n",
    "\n",
    "# Basic statistics for headline length\n",
    "headline_stats = df['headline_length'].describe()\n",
    "\n",
    "# Print the statistics\n",
    "print(headline_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d8b6d1-5d04-4f56-abb4-a091bad36608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c57249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if the 'date' column exists in the DataFrame\n",
    "if 'date' in df.columns:\n",
    "    # Convert the 'date' column to datetime format with the correct format\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce', format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Extract relevant date components for analysis\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "    df['weekday'] = df['date'].dt.weekday  \n",
    "\n",
    "    # Analyze trends over time, such as news frequency by year, month, day, and weekday\n",
    "    news_frequency_by_year = df['year'].value_counts().sort_index()\n",
    "    news_frequency_by_month = df.groupby('month').size()\n",
    "    news_frequency_by_day = df.groupby('day').size()\n",
    "    news_frequency_by_weekday = df.groupby('weekday').size()\n",
    "\n",
    "    # Plotting news frequency trends over time\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    news_frequency_by_year.plot(kind='bar', color='skyblue')\n",
    "    plt.title('News Frequency by Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    news_frequency_by_month.plot(kind='bar', color='salmon')\n",
    "    plt.title('News Frequency by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    news_frequency_by_day.plot(kind='line', color='green')\n",
    "    plt.title('News Frequency by Day')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    news_frequency_by_weekday.plot(kind='bar', color='orange')\n",
    "    plt.title('News Frequency by Weekday')\n",
    "    plt.xlabel('Weekday')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: 'date' column not found in the DataFrame.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd70e907-ec31-4e73-bf40-5b1b0efb403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score for each headline\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)  # Get sentiment scores\n",
    "    return sentiment\n",
    "\n",
    "# Apply the function to each headline and store the results in a new column\n",
    "df['sentiment'] = df['headline'].apply(get_sentiment_score)\n",
    "\n",
    "# Define a function to classify the sentiment based on the compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to the compound score and store the results in a new column\n",
    "df['sentiment_class'] = df['sentiment'].apply(lambda x: classify_sentiment(x['compound']))\n",
    "\n",
    "# Count the number of headlines per sentiment class\n",
    "sentiment_counts = df['sentiment_class'].value_counts()\n",
    "\n",
    "# Print the sentiment counts in a more descriptive format\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"-------------------------\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} headlines\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c4c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000  # Increase max_length to handle longer texts\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Set chunk size\n",
    "chunk_size = 100000  # Adjust as needed\n",
    "\n",
    "# Function to process text in chunks\n",
    "def process_text_chunks(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Perform part-of-speech tagging to extract nouns and proper nouns\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "    # Use spaCy for named entity recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Filter entities to extract only significant ones\n",
    "    significant_entities = [entity[0] for entity in entities if entity[1] in ['ORG', 'PERSON', 'GPE', 'EVENT']]\n",
    "\n",
    "    # Combine both noun phrases and significant named entities\n",
    "    keywords = nouns + significant_entities\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Concatenate all headlines into a single string\n",
    "all_headlines = ' '.join(df['headline'])\n",
    "\n",
    "# Process the text in chunks\n",
    "chunks = [all_headlines[i:i+chunk_size] for i in range(0, len(all_headlines), chunk_size)]\n",
    "\n",
    "# Initialize a list to store keywords from all chunks\n",
    "all_keywords = []\n",
    "\n",
    "# Process each chunk separately\n",
    "for chunk in chunks:\n",
    "    keywords = process_text_chunks(chunk)\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "# Calculate the frequency distribution of keywords\n",
    "fdist = FreqDist(all_keywords)\n",
    "\n",
    "# Get the most common keywords\n",
    "common_keywords = fdist.most_common(10)\n",
    "\n",
    "# Print the most common keywords\n",
    "print(\"Most common keywords or phrases:\")\n",
    "for keyword, frequency in common_keywords:\n",
    "    print(f\"{keyword}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250df10-2ec1-4979-a8c0-b47d9d567537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the data to get the count of articles per day\n",
    "article_count_per_day = df.resample('D').size()\n",
    "\n",
    "# Plot the publication frequency over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "article_count_per_day.plot()\n",
    "plt.title('Publication Frequency Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9658-3fdf-4d64-8975-d221c376eaf3",
   "metadata": {},
   "source": [
    " # Analysis of publishing times might reveal if thereâ€™s a specific time when most news is released, which could be crucial for traders and automated trading systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fae66-03ad-43df-bc42-edba8455f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Extract the hour of the day from the 'date' column\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Count the number of articles published during each hour\n",
    "article_count_by_hour = df.groupby('hour').size()\n",
    "\n",
    "# Plot the distribution of publishing times\n",
    "plt.figure(figsize=(10, 6))\n",
    "article_count_by_hour.plot(kind='bar')\n",
    "plt.title('Distribution of Publishing Times')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(range(24), rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc40c25-a3ab-4caf-a389-1ec53908eafe",
   "metadata": {},
   "source": [
    "# Which publishers contribute most to the news feed? Is there a difference in the type of news they report?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ff4f-53e8-4953-86b3-a3b41dd04faf",
   "metadata": {},
   "source": [
    "# To answer the above two question, we use \n",
    "# 1)findind number of article publishe by each publisher\n",
    "# 2)Number of stop words in each publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902a9bb-2bbe-421f-a574-79647b7a8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Count the number of articles published by each publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# Sort publishers by the number of articles published (in ascending order)\n",
    "sorted_publishers = publisher_counts.sort_values().index\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and process the headlines to identify common keywords\n",
    "keywords_by_publisher = {}\n",
    "for publisher in sorted_publishers:\n",
    "    publisher_data = df[df['publisher'] == publisher]\n",
    "    all_headlines = ' '.join(publisher_data['headline'])\n",
    "    tokens = word_tokenize(all_headlines)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    keywords_by_publisher[publisher] = fdist.most_common(10)  # Get top 10 keywords for each publisher\n",
    "\n",
    "# Print the number of articles published by each publisher\n",
    "print(\"Number of articles published by each publisher (in ascending order):\")\n",
    "print(publisher_counts)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the top keywords associated with each publisher\n",
    "print(\"Top keywords associated with each publisher:\")\n",
    "for publisher in sorted_publishers:\n",
    "    print(f\"Publisher: {publisher}\")\n",
    "    keywords = keywords_by_publisher.get(publisher, [])\n",
    "    for keyword, frequency in keywords:\n",
    "        print(f\"{keyword}: {frequency}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a23c9-792a-435d-b1b1-ab5cf192e0fd",
   "metadata": {},
   "source": [
    "# What if the publisher is an Orginization which contributes most to news?\n",
    "# if eamil is used as publisher name we can extract unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb165f-1cf4-46fe-a839-b44094bf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from email addresses\n",
    "df['domain'] = df['publisher'].str.extract(r'@(.*)')\n",
    "\n",
    "# Count the number of articles published by each domain\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "# Print the unique domains and their corresponding publication counts\n",
    "print(\"Unique domains and their publication counts:\")\n",
    "print(domain_counts)\n",
    "\n",
    "# Find the domain with the highest number of articles published\n",
    "most_contributor_domain = domain_counts.idxmax()\n",
    "articles_published = domain_counts.max()\n",
    "\n",
    "# Print the most contributing domain\n",
    "print(f\"The most contributing domain is '{most_contributor_domain}' with {articles_published} articles published.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f7457-7de2-48dc-827d-ac954723b32f",
   "metadata": {},
   "source": [
    "# The most contributing domain is 'benzinga.com' with 7937 articles published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841dfac-6951-43b0-b802-6564aa134617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Load the dataset containing stock symbols and dates from a CSV file into a pandas DataFrame\n",
    "\n",
    "# Convert the 'date' column to datetime objects to facilitate date manipulation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Determine date ranges and fetch stock data\n",
    "date_ranges = df.groupby('symbol').agg({'date': ['min', 'max']})\n",
    "\n",
    "# Function to fetch stock data for a given symbol and date range\n",
    "def fetch_stock_data(symbol, start_date, end_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through each symbol and fetch stock data based on the calculated date ranges\n",
    "for symbol, (start_date, end_date) in date_ranges.iterrows():\n",
    "    print(f\"Fetching data for {symbol} from {start_date} to {end_date}\")\n",
    "    stock_data = fetch_stock_data(symbol, start_date, end_date)\n",
    "    stock_data\n",
    "    \n",
    "    # Process fetched data (you can save it, manipulate it, etc.)\n",
    "    if stock_data is not None:\n",
    "        # Perform operations on the fetched data here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4cab0-94be-4227-b93b-ce7b14719f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b139cb-a33a-467c-80da-64079e149b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea465be-1e8e-4dbd-a734-f2937c33b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6872d8e3-a1d2-4f13-819e-ce68d171c4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
