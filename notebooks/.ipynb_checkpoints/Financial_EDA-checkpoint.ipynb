{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a4ac7-bbc7-4b5a-93f8-b4c2a2716b58",
   "metadata": {},
   "source": [
    "# Obtaining basic statistics for textual lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d62a9927-f2fc-440c-b877-1a77aeade405",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'talib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myfinance\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01myf\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtalib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mta\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'talib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import talib as ta\n",
    "import nltk \n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31410586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96fda116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yadasa/Desktop/Finance-Data-Analysis/data/raw_analyst_ratings.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1213658-409d-4fbf-b3fe-d72966482e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd18e761-800e-4cb8-9397-878df59a5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of headlines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.407328e+06\n",
       "mean     7.312051e+01\n",
       "std      4.073531e+01\n",
       "min      3.000000e+00\n",
       "25%      4.700000e+01\n",
       "50%      6.400000e+01\n",
       "75%      8.700000e+01\n",
       "max      5.120000e+02\n",
       "Name: headline_length, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate the length of each headline and create a new column\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(x))\n",
    "\n",
    "# Basic statistics for headline length\n",
    "headline_stats = df['headline_length'].describe()\n",
    "\n",
    "# Print the statistics\n",
    "print(\"\\nStatistics of headlines\")\n",
    "\n",
    "headline_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54c57249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News frequency by year:\n",
      "year\n",
      "2009     11489\n",
      "2010     81319\n",
      "2011    131322\n",
      "2012    122655\n",
      "2013    121523\n",
      "2014    134859\n",
      "2015    135295\n",
      "2016    141892\n",
      "2017    124456\n",
      "2018    146924\n",
      "2019    150380\n",
      "2020    105214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published each year.\n",
      "\n",
      "News frequency by month:\n",
      "month\n",
      "1     121541\n",
      "2     122835\n",
      "3     121948\n",
      "4     121815\n",
      "5     130339\n",
      "6     106600\n",
      "7     110762\n",
      "8     124042\n",
      "9      96087\n",
      "10    124800\n",
      "11    121431\n",
      "12    105128\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each month of the year.\n",
      "\n",
      "News frequency by day:\n",
      "day\n",
      "1     46573\n",
      "2     46905\n",
      "3     45782\n",
      "4     45358\n",
      "5     47123\n",
      "6     50039\n",
      "7     50073\n",
      "8     50682\n",
      "9     48692\n",
      "10    47793\n",
      "11    46064\n",
      "12    48610\n",
      "13    47769\n",
      "14    46714\n",
      "15    44633\n",
      "16    44206\n",
      "17    44610\n",
      "18    43857\n",
      "19    44918\n",
      "20    46435\n",
      "21    44597\n",
      "22    43161\n",
      "23    46515\n",
      "24    44040\n",
      "25    43007\n",
      "26    45571\n",
      "27    47574\n",
      "28    45049\n",
      "29    41787\n",
      "30    42008\n",
      "31    27183\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each day of the month.\n",
      "\n",
      "News frequency by weekday:\n",
      "weekday\n",
      "0    265164\n",
      "1    296470\n",
      "2    300940\n",
      "3    302595\n",
      "4    217892\n",
      "5      7753\n",
      "6     16514\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each day of the week (Monday to Sunday). Monday is represented as 0, Sunday as 6.\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Extract relevant date components for analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.weekday  \n",
    "# Monday is 0, Sunday is 6\n",
    "\n",
    "# Analyze trends over time, such as news frequency by year, month, day, and weekday\n",
    "news_frequency_by_year = df['year'].value_counts().sort_index()\n",
    "news_frequency_by_month = df.groupby('month')['headline'].count()\n",
    "news_frequency_by_day = df.groupby('day')['headline'].count()\n",
    "news_frequency_by_weekday = df.groupby('weekday')['headline'].count()\n",
    "\n",
    "# Print descriptive output\n",
    "print(\"News frequency by year:\")\n",
    "print(news_frequency_by_year)\n",
    "print(\"\\nExplanation: This shows the count of news articles published each year.\")\n",
    "\n",
    "print(\"\\nNews frequency by month:\")\n",
    "print(news_frequency_by_month)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each month of the year.\")\n",
    "\n",
    "print(\"\\nNews frequency by day:\")\n",
    "print(news_frequency_by_day)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the month.\")\n",
    "\n",
    "print(\"\\nNews frequency by weekday:\")\n",
    "print(news_frequency_by_weekday)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the week (Monday to Sunday). Monday is represented as 0, Sunday as 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e347b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "-------------------------\n",
      "Neutral: 741194 headlines\n",
      "Positive: 441858 headlines\n",
      "Negative: 224276 headlines\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score for each headline\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Apply the function to each headline and store the results in a new column\n",
    "df['sentiment'] = df['headline'].apply(get_sentiment_score)\n",
    "\n",
    "# Define a function to classify the sentiment based on the compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to the compound score and store the results in a new column\n",
    "df['sentiment_class'] = df['sentiment'].apply(lambda x: classify_sentiment(x['compound']))\n",
    "\n",
    "# Count the number of headlines per sentiment class\n",
    "sentiment_counts = df['sentiment_class'].value_counts()\n",
    "\n",
    "# Print the sentiment counts in a more descriptive format\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"-------------------------\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} headlines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea8c4c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common keywords or phrases:\n",
      "Stocks: 157529\n",
      "EPS: 136718\n",
      "Est: 120858\n",
      "Reports: 106667\n",
      "vs: 104958\n",
      "Benzinga: 92032\n",
      "Shares: 91631\n",
      "Earnings: 84910\n",
      "Market: 83721\n",
      "Top: 75682\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Load the English tokenizer, tagger, parser, and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000  # Increase max_length to handle longer texts\n",
    "\n",
    "# Load English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Set chunk size\n",
    "chunk_size = 100000  # Adjust as needed\n",
    "\n",
    "# Function to process text in chunks\n",
    "def process_text_chunks(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter out stop words and punctuation\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    # Perform part-of-speech tagging to extract nouns and proper nouns\n",
    "    pos_tags = pos_tag(filtered_tokens)\n",
    "    nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "    # Use spaCy for named entity recognition (NER)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    # Filter entities to extract only significant ones\n",
    "    significant_entities = [entity[0] for entity in entities if entity[1] in ['ORG', 'PERSON', 'GPE', 'EVENT']]\n",
    "\n",
    "    # Combine both noun phrases and significant named entities\n",
    "    keywords = nouns + significant_entities\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# Concatenate all headlines into a single string\n",
    "all_headlines = ' '.join(df['headline'])\n",
    "\n",
    "# Process the text in chunks\n",
    "chunks = [all_headlines[i:i+chunk_size] for i in range(0, len(all_headlines), chunk_size)]\n",
    "\n",
    "# Initialize a list to store keywords from all chunks\n",
    "all_keywords = []\n",
    "\n",
    "# Process each chunk separately\n",
    "for chunk in chunks:\n",
    "    keywords = process_text_chunks(chunk)\n",
    "    all_keywords.extend(keywords)\n",
    "\n",
    "# Calculate the frequency distribution of keywords\n",
    "fdist = FreqDist(all_keywords)\n",
    "\n",
    "# Get the most common keywords\n",
    "common_keywords = fdist.most_common(10)\n",
    "\n",
    "# Print the most common keywords\n",
    "print(\"Most common keywords or phrases:\")\n",
    "for keyword, frequency in common_keywords:\n",
    "    print(f\"{keyword}: {frequency}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3250df10-2ec1-4979-a8c0-b47d9d567537",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the 'date' column to datetime format\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Set the 'date' column as the index\u001b[39;00m\n\u001b[1;32m      5\u001b[0m df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "f['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the data to get the count of articles per day\n",
    "article_count_per_day = df.resample('D').size()\n",
    "\n",
    "# Plot the publication frequency over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "article_count_per_day.plot()\n",
    "plt.title('Publication Frequency Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafc9658-3fdf-4d64-8975-d221c376eaf3",
   "metadata": {},
   "source": [
    " # Analysis of publishing times might reveal if there’s a specific time when most news is released, which could be crucial for traders and automated trading systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f29fae66-03ad-43df-bc42-edba8455f206",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# with a column named 'date' containing the publication dates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convert the 'date' column to datetime format\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract the hour of the day from the 'date' column\u001b[39;00m\n\u001b[1;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# with a column named 'date' containing the publication dates.\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Extract the hour of the day from the 'date' column\n",
    "df['hour'] = df['date'].dt.hour\n",
    "\n",
    "# Count the number of articles published during each hour\n",
    "article_count_by_hour = df.groupby('hour').size()\n",
    "\n",
    "# Plot the distribution of publishing times\n",
    "plt.figure(figsize=(10, 6))\n",
    "article_count_by_hour.plot(kind='bar')\n",
    "plt.title('Distribution of Publishing Times')\n",
    "plt.xlabel('Hour of the Day')\n",
    "plt.ylabel('Number of Articles Published')\n",
    "plt.xticks(range(24), rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc40c25-a3ab-4caf-a389-1ec53908eafe",
   "metadata": {},
   "source": [
    "# Which publishers contribute most to the news feed? Is there a difference in the type of news they report?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e652ff4f-53e8-4953-86b3-a3b41dd04faf",
   "metadata": {},
   "source": [
    "# To answer the above two question, we use \n",
    "# 1)findind number of article publishe by each publisher\n",
    "# 2)Number of stop words in each publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902a9bb-2bbe-421f-a574-79647b7a8805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Count the number of articles published by each publisher\n",
    "publisher_counts = df['publisher'].value_counts()\n",
    "\n",
    "# Sort publishers by the number of articles published (in ascending order)\n",
    "sorted_publishers = publisher_counts.sort_values().index\n",
    "\n",
    "# Filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and process the headlines to identify common keywords\n",
    "keywords_by_publisher = {}\n",
    "for publisher in sorted_publishers:\n",
    "    publisher_data = df[df['publisher'] == publisher]\n",
    "    all_headlines = ' '.join(publisher_data['headline'])\n",
    "    tokens = word_tokenize(all_headlines)\n",
    "    filtered_tokens = [word.lower() for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "    fdist = FreqDist(filtered_tokens)\n",
    "    keywords_by_publisher[publisher] = fdist.most_common(10)  # Get top 10 keywords for each publisher\n",
    "\n",
    "# Print the number of articles published by each publisher\n",
    "print(\"Number of articles published by each publisher (in ascending order):\")\n",
    "print(publisher_counts)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the top keywords associated with each publisher\n",
    "print(\"Top keywords associated with each publisher:\")\n",
    "for publisher in sorted_publishers:\n",
    "    print(f\"Publisher: {publisher}\")\n",
    "    keywords = keywords_by_publisher.get(publisher, [])\n",
    "    for keyword, frequency in keywords:\n",
    "        print(f\"{keyword}: {frequency}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a23c9-792a-435d-b1b1-ab5cf192e0fd",
   "metadata": {},
   "source": [
    "# What if the publisher is an Orginization which contributes most to news?\n",
    "# if eamil is used as publisher name we can extract unique domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb165f-1cf4-46fe-a839-b44094bf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract domain from email addresses\n",
    "df['domain'] = df['publisher'].str.extract(r'@(.*)')\n",
    "\n",
    "# Count the number of articles published by each domain\n",
    "domain_counts = df['domain'].value_counts()\n",
    "\n",
    "# Print the unique domains and their corresponding publication counts\n",
    "print(\"Unique domains and their publication counts:\")\n",
    "print(domain_counts)\n",
    "\n",
    "# Find the domain with the highest number of articles published\n",
    "most_contributor_domain = domain_counts.idxmax()\n",
    "articles_published = domain_counts.max()\n",
    "\n",
    "# Print the most contributing domain\n",
    "print(f\"The most contributing domain is '{most_contributor_domain}' with {articles_published} articles published.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0f7457-7de2-48dc-827d-ac954723b32f",
   "metadata": {},
   "source": [
    "# The most contributing domain is 'benzinga.com' with 7937 articles published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841dfac-6951-43b0-b802-6564aa134617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "# Load the dataset containing stock symbols and dates from a CSV file into a pandas DataFrame\n",
    "\n",
    "# Convert the 'date' column to datetime objects to facilitate date manipulation\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Determine date ranges and fetch stock data\n",
    "date_ranges = df.groupby('symbol').agg({'date': ['min', 'max']})\n",
    "\n",
    "# Function to fetch stock data for a given symbol and date range\n",
    "def fetch_stock_data(symbol, start_date, end_date):\n",
    "    try:\n",
    "        # Fetch stock data using yfinance\n",
    "        stock_data = yf.download(symbol, start=start_date, end=end_date)\n",
    "        return stock_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through each symbol and fetch stock data based on the calculated date ranges\n",
    "for symbol, (start_date, end_date) in date_ranges.iterrows():\n",
    "    print(f\"Fetching data for {symbol} from {start_date} to {end_date}\")\n",
    "    stock_data = fetch_stock_data(symbol, start_date, end_date)\n",
    "    stock_data\n",
    "    \n",
    "    # Process fetched data (you can save it, manipulate it, etc.)\n",
    "    if stock_data is not None:\n",
    "        # Perform operations on the fetched data here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc8c63-a7f9-4aa0-ac56-368897e3bab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7dad9-f7dd-454d-9866-75af71f3960a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca7ab9-8d8a-4576-a5a7-0dd65c2210a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df1a14-8176-4331-9f8f-e2acf44a76a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
