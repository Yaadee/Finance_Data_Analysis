{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd2a4ac7-bbc7-4b5a-93f8-b4c2a2716b58",
   "metadata": {},
   "source": [
    "# Obtaining basic statistics for textual lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b261be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31410586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34686119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491e4951",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/yadasa/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa45ef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff07052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d0e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfed4baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcebef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/yadasa/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96fda116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/yadasa/Desktop/Finance-Data-Analysis/data/raw_analyst_ratings.csv\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd18e761-800e-4cb8-9397-878df59a5a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of headlines\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    1.407328e+06\n",
       "mean     7.312051e+01\n",
       "std      4.073531e+01\n",
       "min      3.000000e+00\n",
       "25%      4.700000e+01\n",
       "50%      6.400000e+01\n",
       "75%      8.700000e+01\n",
       "max      5.120000e+02\n",
       "Name: headline_length, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Calculate the length of each headline and create a new column\n",
    "df['headline_length'] = df['headline'].apply(lambda x: len(x))\n",
    "\n",
    "# Basic statistics for headline length\n",
    "headline_stats = df['headline_length'].describe()\n",
    "\n",
    "# Print the statistics\n",
    "print(\"\\nStatistics of headlines\")\n",
    "\n",
    "headline_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54c57249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News frequency by year:\n",
      "year\n",
      "2009     11489\n",
      "2010     81319\n",
      "2011    131322\n",
      "2012    122655\n",
      "2013    121523\n",
      "2014    134859\n",
      "2015    135295\n",
      "2016    141892\n",
      "2017    124456\n",
      "2018    146924\n",
      "2019    150380\n",
      "2020    105214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published each year.\n",
      "\n",
      "News frequency by month:\n",
      "month\n",
      "1     121541\n",
      "2     122835\n",
      "3     121948\n",
      "4     121815\n",
      "5     130339\n",
      "6     106600\n",
      "7     110762\n",
      "8     124042\n",
      "9      96087\n",
      "10    124800\n",
      "11    121431\n",
      "12    105128\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each month of the year.\n",
      "\n",
      "News frequency by day:\n",
      "day\n",
      "1     46573\n",
      "2     46905\n",
      "3     45782\n",
      "4     45358\n",
      "5     47123\n",
      "6     50039\n",
      "7     50073\n",
      "8     50682\n",
      "9     48692\n",
      "10    47793\n",
      "11    46064\n",
      "12    48610\n",
      "13    47769\n",
      "14    46714\n",
      "15    44633\n",
      "16    44206\n",
      "17    44610\n",
      "18    43857\n",
      "19    44918\n",
      "20    46435\n",
      "21    44597\n",
      "22    43161\n",
      "23    46515\n",
      "24    44040\n",
      "25    43007\n",
      "26    45571\n",
      "27    47574\n",
      "28    45049\n",
      "29    41787\n",
      "30    42008\n",
      "31    27183\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each day of the month.\n",
      "\n",
      "News frequency by weekday:\n",
      "weekday\n",
      "0    265164\n",
      "1    296470\n",
      "2    300940\n",
      "3    302595\n",
      "4    217892\n",
      "5      7753\n",
      "6     16514\n",
      "Name: headline, dtype: int64\n",
      "\n",
      "Explanation: This shows the count of news articles published for each day of the week (Monday to Sunday). Monday is represented as 0, Sunday as 6.\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='ISO8601')\n",
    "\n",
    "# Extract relevant date components for analysis\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['weekday'] = df['date'].dt.weekday  \n",
    "# Monday is 0, Sunday is 6\n",
    "\n",
    "# Analyze trends over time, such as news frequency by year, month, day, and weekday\n",
    "news_frequency_by_year = df['year'].value_counts().sort_index()\n",
    "news_frequency_by_month = df.groupby('month')['headline'].count()\n",
    "news_frequency_by_day = df.groupby('day')['headline'].count()\n",
    "news_frequency_by_weekday = df.groupby('weekday')['headline'].count()\n",
    "\n",
    "# Print descriptive output\n",
    "print(\"News frequency by year:\")\n",
    "print(news_frequency_by_year)\n",
    "print(\"\\nExplanation: This shows the count of news articles published each year.\")\n",
    "\n",
    "print(\"\\nNews frequency by month:\")\n",
    "print(news_frequency_by_month)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each month of the year.\")\n",
    "\n",
    "print(\"\\nNews frequency by day:\")\n",
    "print(news_frequency_by_day)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the month.\")\n",
    "\n",
    "print(\"\\nNews frequency by weekday:\")\n",
    "print(news_frequency_by_weekday)\n",
    "print(\"\\nExplanation: This shows the count of news articles published for each day of the week (Monday to Sunday). Monday is represented as 0, Sunday as 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e347b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results:\n",
      "-------------------------\n",
      "Neutral: 741194 headlines\n",
      "Positive: 441858 headlines\n",
      "Negative: 224276 headlines\n"
     ]
    }
   ],
   "source": [
    "# Initialize the VADER sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to get the sentiment score for each headline\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "# Apply the function to each headline and store the results in a new column\n",
    "df['sentiment'] = df['headline'].apply(get_sentiment_score)\n",
    "\n",
    "# Define a function to classify the sentiment based on the compound score\n",
    "def classify_sentiment(score):\n",
    "    if score >= 0.05:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "# Apply the function to the compound score and store the results in a new column\n",
    "df['sentiment_class'] = df['sentiment'].apply(lambda x: classify_sentiment(x['compound']))\n",
    "\n",
    "# Count the number of headlines per sentiment class\n",
    "sentiment_counts = df['sentiment_class'].value_counts()\n",
    "\n",
    "# Print the sentiment counts in a more descriptive format\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "print(\"-------------------------\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    print(f\"{sentiment}: {count} headlines\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea8c4c1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 104311875 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m nouns \u001b[38;5;241m=\u001b[39m [word \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_tags \u001b[38;5;28;01mif\u001b[39;00m pos\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNN\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Use spaCy for named entity recognition (NER)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_headlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m entities \u001b[38;5;241m=\u001b[39m [(ent\u001b[38;5;241m.\u001b[39mtext, ent\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Filter entities to extract only significant ones\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:1128\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py:1117\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m-> 1117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1118\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1119\u001b[0m     )\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 104311875 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the English tokenizer, tagger, parser, NER, and stop words\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# with a column named 'headline' containing the headlines.\n",
    "\n",
    "# Concatenate all headlines into a single string\n",
    "all_headlines = ' '.join(df['headline'])\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(all_headlines)\n",
    "\n",
    "# Filter out stop words and punctuation\n",
    "filtered_tokens = [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "# Perform part-of-speech tagging to extract nouns and proper nouns\n",
    "pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "# Use spaCy for named entity recognition (NER)\n",
    "doc = nlp(all_headlines)\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Filter entities to extract only significant ones\n",
    "significant_entities = [entity[0] for entity in entities if entity[1] in ['ORG', 'PERSON', 'GPE', 'EVENT']]\n",
    "\n",
    "# Combine both noun phrases and significant named entities\n",
    "keywords = nouns + significant_entities\n",
    "\n",
    "# Calculate the frequency distribution of keywords\n",
    "fdist = FreqDist(keywords)\n",
    "\n",
    "# Get the most common keywords\n",
    "common_keywords = fdist.most_common(10)\n",
    "\n",
    "# Print the most common keywords\n",
    "print(\"Most common keywords or phrases:\")\n",
    "for keyword, frequency in common_keywords:\n",
    "    print(f\"{keyword}: {frequency}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
